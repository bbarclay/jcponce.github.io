<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8" />

    <title>MATH2001/7000 - Chapter 12</title>

    <meta
      name="viewport"
      content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no"
    />

    <link rel="stylesheet" href="dist/reveal.css" />
    <link rel="stylesheet" href="dist/theme/white.css" id="theme" />
    <link rel="stylesheet" href="plugin/chalkboard/style.css" />
    <link rel="stylesheet" href="plugin/customcontrols/style.css" />

    <link rel="stylesheet" href="css/mylayout.css" />

    <!-- Code syntax highlighting https://revealjs.com/-->
    <link rel="stylesheet" href="plugin/highlight/zenburn.css" />

    <!-- Font awesome -->
    <link rel="stylesheet" href="css/fontawesome.css" />

    <link
      rel="icon"
      type="image/png"
      href="images/icon/infinity32.png"
      sizes="32x32"
    />
    <link
      rel="icon"
      type="image/png"
      href="images/icon/infinity16.png"
      sizes="16x16"
    />

    <!-- Printing and PDF exports -->
    <script>
      var link = document.createElement("link");
      link.rel = "stylesheet";
      link.type = "text/css";
      link.href = window.location.search.match(/print-pdf/gi)
        ? "export/pdf.css"
        : "export/paper.css";
      document.getElementsByTagName("head")[0].appendChild(link);
    </script>

    <!-- This script is to display Github buttons -->
    <script async defer src="js/buttons.js"></script>

    <!--[if lt IE 9]>
      <script src="../reveal.js/lib/js/html5shiv.js"></script>
    <![endif]-->

    <!-- Global site tag (gtag.js) - Google Analytics -->
    <script
      async
      src="https://www.googletagmanager.com/gtag/js?id=G-JPYTNF6MB4"
    ></script>
    <script src="js/google-analytics-ga4.js"></script>
  </head>

  <body>
    <div class="reveal">
      <div class="slides">
        <!-- Any section element inside of this container is displayed as a slide -->

        <section>
          <h2>Calculus & <br />Linear Algebra II</h2>
          <p>Chapter 12</p>
        </section>

        <section>
          <h3>12 Least squares approximation</h3>
          <br /><br /><br /><br /><br />
        </section>
 
        <section>
          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>
            <div class="left-txt size-65">
              <p class="fragment" data-fragment-index="1">
                A recurring problem in linear algebra, and in its myriad of
                applications, is the following:
              </p>
              <ul class="fragment" data-fragment-index="2">
                <li>
                  Given a vector ${\bf v}$ in a real inner product space $V$,
                  give the best approximation to ${\bf v}$ in a
                  finite-dimensional subspace $U$ of $V$.
                </li>
              </ul>
              <p class="fragment" data-fragment-index="3">
                This problem is called the "least squares problem".
              </p>
            </div>

            <div class="left-txt size-65">
              <p class="fragment" data-fragment-index="4">
                What do we mean by "best approximation"?
                <span class="fragment" data-fragment-index="5">
                  It means to find a vector in a subspace of minimal distance to
                  a given vector in the ambient vector space.
                </span>
                <span class="fragment" data-fragment-index="6">
                  So the answer to the problem is to
                </span>
              </p>
            </div>

            <div class="size-65">
              <p class="fragment" data-fragment-index="6">
                <em
                  >find $\bfu\in U$ such that $d(\bfu,\bfv)$ is as small as
                  possible.</em
                >
              </p>
            </div>
            <br />
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>

            <div class="left-txt size-60">
              <p>
                What do we mean by "best approximation"?
                <span>
                  It means to find a vector in a subspace of minimal distance to
                  a given vector in the ambient vector space.
                </span>
                <span>
                  So the answer to the problem is to
                </span>
              </p>
            </div>

            <div class="size-60">
              <p>
                <em
                  >find $\bfu\in U$ such that $d(\bfu,\bfv)$ is as small as
                  possible.</em
                >
              </p>
            </div>

            <div class="left-txt size-60">
              <p>
                That is, $\mathbf{u}\in U$ that minimises
                $||\mathbf{v}-\mathbf{u}||.$
              </p>
            </div>

            <div class="left-txt size-65">
              <div class="fragment" data-fragment-index="0">
                <p>
                  <strong>Theorem </strong>(Best Approximation Theorem). If $U$
                  is a finite-dimensional subspace of a real inner product space
                  $V$, and if $\mathbf{v}\in V,$ then $\mathrm{Proj}_U(\bfv)$ is
                  the best approximation to $\mathbf{v}$ from $U$ in the sense
                  that $$||\mathbf{v}-\mathrm{Proj}_U(\bfv)|| \lt
                  ||\mathbf{v}-\mathbf{u}||, ~~\forall \mathbf{u}\in U.$$
                </p>
              </div>
            </div>
            <br /><br />
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>
            <div>
              <iframe scrolling="no" title="Chapter-11-2: Theorem" 
              src="https://www.geogebra.org/material/iframe/id/rhscbubs/width/807/height/492/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/true" 
              width="807px" height="492px" style="border:0px;" allowfullscreen=""> </iframe>
             
            </div>
            <br/><br/>
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>
            <div class="left-txt" style="font-size:40%">
              <div>
                <p>
                  <strong>Theorem </strong>(Best Approximation Theorem). If $U$
                  is a finite-dimensional subspace of a real inner product space
                  $V$, and if $\mathbf{v}\in V,$ then $\mathrm{Proj}_U(\bfv)$ is
                  the best approximation to $\mathbf{v}$ from $U$ in the sense
                  that $$||\mathbf{v}-\mathrm{Proj}_U(\bfv)|| \lt
                  ||\mathbf{v}-\mathbf{u}||, ~~\forall \mathbf{u}\in U.$$
                </p>
              </div>
            </div>
            <div class="size-70">
              <div class="left-txt">
                <p><strong>Proof:</strong>
                  <span class="fragment" data-fragment-index="0">
                    Let $U$ be a finite-dimensional subspace of $V$ and $\v \in V$, $\u\in U$.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Then 
                  </span>
                </p>
              </div>
              <div class="r-stack">
                <p class="fragment fade-in-then-out" data-fragment-index="1">
                  $
                  \norm{ \v - \mathrm{Proj}_U(\bfv) }^2 
                  \leq
                  \norm{ \v - \mathrm{Proj}_U(\bfv) }^2+ \norm{ \mathrm{Proj}_U(\bfv) -\u }^2$
                </p>
                <p class="fragment fade-in" data-fragment-index="2">
                  $
                  \norm{ \v - \mathrm{Proj}_U(\bfv) }^2 
                  \leq
                  \underbrace{\norm{ \v - \mathrm{Proj}_U(\bfv) }^2}_{\in U^{\perp}}+ 
                  \underbrace{\norm{ \mathrm{Proj}_U(\bfv) -\u }^2}_{\in U}$
                </p>
                
              </div>

              <div>
                <p class="fragment" data-fragment-index="3">
                  $
                  \qquad\quad\qquad
                  \leq
                  \norm{ \v - \mathrm{Proj}_U(\bfv) 
                  +
                   \mathrm{Proj}_U(\bfv) -\u }^2
                  $
                </p>
                <p class="fragment" data-fragment-index="4">
                  $
                  
                  \leq \norm{ \v  -\u }^2.\qquad\quad\,
                  $
                </p>
              </div>
              <div>
                <p class="fragment" data-fragment-index="5">
                  Therefore $\norm{ \v - \mathrm{Proj}_U(\bfv) } \leq \norm{ \v  -\u }$ for all $\u\in U. \qquad
                  \blacksquare$
                </p>
              </div>
            </div>
            
            <br/><br/>
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>

            <div class="left-txt size-60">
              <p>
                From the Best Approximation Theorem, $\u =
                \mathrm{Proj}_U(\bfv)$ is the best approximation to $\v \in V$
                in a finite-dimensional subspace $U$ of $V$.
              </p>
              <p>
                <span class="fragment" data-fragment-index="0">
                  <em>How to find $\mathrm{Proj}_U(\bfv)$?</em>
                </span>
              </p>
              <p>
                <span  class="fragment" data-fragment-index="1">
                  <strong>Solution 1.</strong>
                Use Gram-Schmidt process to construct an orthonormal basis
                $\{\bfe_1, \bfe_2,\cdots,\bfe_n\}$ of $U$.
                </span>
                <span class="fragment" data-fragment-index="2">
                  Then $$ \mathrm{Proj}_U(\bfv)=
                  \langle\bfv,\bfe_1\rangle\bfe_1+\ldots+\langle\bfv,\bfe_n\rangle\bfe_n.
                  $$
                </span>
              </p>
              <p class="fragment" data-fragment-index="3">
                <strong>Solution 2.</strong>
                Let $\gamma=\{\bfu_1, \bfu_2,\cdots,\bfu_n\}$ be a basis of the
                subspace $U$.
                <span class="fragment" data-fragment-index="4">
                  Then any $\u \in U$ can be written as $\u = \alpha_1 \u_1+ \alpha_2
                  \u_2+ \cdots \alpha_n \u_n$.
                </span>
              </p>
            </div>
            <br /><br />
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>

            <div class="left-txt size-60">
              <p>
                <strong>Solution 2.</strong>
                Let $\gamma=\{\bfu_1, \bfu_2,\cdots,\bfu_n\}$ be a basis of the
                subspace $U$.
                <span>
                  Then any $\u \in U$ can be written as $\u = \alpha_1 \u_1+ \alpha_2
                  \u_2+ \cdots \alpha_n \u_n$.
                </span>
              </p>
            </div>

            <div class="left-txt size-60">
              <p>
                We seek coefficients $\alpha_1,\alpha_2,\dots \alpha_n$ that
                minimise $||\mathbf{v}-\u||$,
                <span class="fragment" data-fragment-index="0">
                  or equivalently, minimise
                  $||\mathbf{v}-\u||^2=||\mathbf{v}-(\alpha_1
                  \mathbf{u}_1+\alpha_2 \mathbf{u}_2+\dots +\alpha_n
                  \mathbf{u}_n)||^2$ (same outcome, avoid square root).
                </span>
              </p>
              <p
                style="margin-bottom: 5px;"
                class="fragment"
                data-fragment-index="1"
              >
                $||\mathbf{v}-(\alpha_1 \mathbf{u}_1+\alpha_2 \mathbf{u}_2+\dots
                +\alpha_n \mathbf{u}_n)||^2$
              </p>
              <table id="eqarray">
                <tr class="fragment" data-fragment-index="2">
                  <td>
                    $\quad$
                  </td>
                  <td>
                    $=\langle \mathbf{v}-(\alpha_1 \mathbf{u}_1+\alpha_2
                    \mathbf{u}_2+\dots +\alpha_n \mathbf{u}_n),
                    \mathbf{v}-(\alpha_1 \mathbf{u}_1+\alpha_2
                    \mathbf{u}_2+\dots +\alpha_n \mathbf{u}_n) \rangle$
                  </td>
                </tr>
                <tr class="fragment" data-fragment-index="3">
                  <td>
                    $\quad$
                  </td>
                  <td>
                    $=\langle \mathbf{v},\mathbf{v} \rangle-2\alpha_1 \langle
                    \mathbf{v},\mathbf{u}_1 \rangle -2\alpha_2 \langle
                    \mathbf{v},\mathbf{u}_2 \rangle -\dots-2\alpha_n \langle
                    \mathbf{v},\mathbf{u}_n \rangle
                    +\sum_{i=1}^n\sum_{j=1}^n\alpha_i\alpha_j \langle
                    \mathbf{u}_i, \mathbf{u}_j \rangle$
                  </td>
                </tr>
                <tr class="fragment" data-fragment-index="4">
                  <td>
                    $\quad$
                  </td>
                  <td>
                    $=E(\alpha_1,\alpha_2,\dots,\alpha_n).$
                  </td>
                </tr>
              </table>
              <p class="fragment" data-fragment-index="5">
                $E$ is a real valued function and is called the "error".
              </p>
            </div>

            <br />
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>

            <div class="left-txt size-60">
              <p>
                From MATH1052/MATH1072: we minimise $E$. Set $\nabla
                E=\mathbf{0}$, which gives
                <span class="fragment" data-fragment-index="0">
                  \begin{align*} \frac{\partial E}{\partial \alpha_k}=-2\langle
                  \mathbf{v},\mathbf{u}_k \rangle+2\sum_{l=1}^n \alpha_l
                  \langle\mathbf{u}_k,\mathbf{u}_l \rangle=0. \end{align*}
                </span>
              </p>
              <p class="fragment" data-fragment-index="1">
                This is $n$ equations in $n$ unknowns,
                <span class="fragment" data-fragment-index="2">
                  which may be expressed in matrix form: \begin{align*} \left(
                  \begin{array}{cccc} \langle \mathbf{u}_1,\mathbf{u}_1\rangle ~
                  & \langle \mathbf{u}_1,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_1,\mathbf{u}_n\rangle \\ \langle
                  \mathbf{u}_2,\mathbf{u}_1\rangle ~ & \langle
                  \mathbf{u}_2,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_2,\mathbf{u}_n\rangle\\ \vdots&~\vdots & \\ \langle
                  \mathbf{u}_n,\mathbf{u}_1\rangle ~ & \langle
                  \mathbf{u}_n,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_n,\mathbf{u}_n\rangle \end{array} \right)
                  \left(\begin{array}{c} \alpha_1\\ \alpha_2\\ \vdots\\ \alpha_n
                  \end{array}\right)&= \left(\begin{array}{c} \langle
                  \mathbf{v},\mathbf{u}_1 \rangle\\ \langle
                  \mathbf{v},\mathbf{u}_2 \rangle \\ \vdots\\ \langle
                  \mathbf{v},\mathbf{u}_n \rangle \end{array}\right).
                  \end{align*}
                </span>
              </p>
            </div>

            <br /><br/>
          </section>

          <section data-auto-animate>
            <h4 style="font-size: 75%;">
              12.1 Least squares problem - minimising distance to a subspace
            </h4>

            <div class="left-txt size-60">
              <p>
                This is $n$ equations in $n$ unknowns,
                <span>
                  which may be expressed in matrix form: \begin{align*} \left(
                  \begin{array}{cccc} \langle \mathbf{u}_1,\mathbf{u}_1\rangle ~
                  & \langle \mathbf{u}_1,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_1,\mathbf{u}_n\rangle \\ \langle
                  \mathbf{u}_2,\mathbf{u}_1\rangle ~ & \langle
                  \mathbf{u}_2,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_2,\mathbf{u}_n\rangle\\ \vdots&~\vdots & \\ \langle
                  \mathbf{u}_n,\mathbf{u}_1\rangle ~ & \langle
                  \mathbf{u}_n,\mathbf{u}_2\rangle ~\dots ~ \langle
                  \mathbf{u}_n,\mathbf{u}_n\rangle \end{array} \right)
                  \left(\begin{array}{c} \alpha_1\\ \alpha_2\\ \vdots\\ \alpha_n
                  \end{array}\right)&= \left(\begin{array}{c} \langle
                  \mathbf{v},\mathbf{u}_1 \rangle\\ \langle
                  \mathbf{v},\mathbf{u}_2 \rangle \\ \vdots\\ \langle
                  \mathbf{v},\mathbf{u}_n \rangle \end{array}\right).
                  \end{align*}
                </span>
              </p>
              <p class="fragment" data-fragment-index="0">
                Solve this system for $\alpha_1, \alpha_2, \ldots,\alpha_n$ to
                obtain \[ \mathrm{Proj}_U(\bfv) = \alpha_1\u_1+
                \alpha_2\u_2+\cdots + \alpha_n\u_n. \]
              </p>

              <p class="fragment" data-fragment-index="1">
                <em>Note:</em> If $\gamma$ is orthonormal $\implies$ matrix on
                LHS$\,=I$ (the identity matrix)
                <span class="fragment" data-fragment-index="2">
                  $\implies$ $\alpha_i=\langle \mathbf{v}, \mathbf{u}_i\rangle$,
                  which coincides with <a href="#/2/2/2">Solution 1</a>, as
                  expected.
                </span>
              </p>
            </div>

            <br />
          </section>
        </section>

        <section>
          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span class="fragment" data-fragment-index="0">
                    In applications, one is often faced with over-determined
                    linear systems. 
                  </span>
                  <span class="fragment" data-fragment-index="1">
                     For example, we may have a bunch of data
                    points that we have reasons to believe should fit on a
                    straight line. 
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    But real-life data points rarely match
                    predictions exactly.
                  </span>
                </p>
                <p class="fragment" data-fragment-index="3">
                  The goal in this section is to develop a method for obtaining
                  the best fit or approximation of a specified kind to a given
                  set of data points.
                </p>
              </div>
            </div>
            <br/><br/><br/><br/>
          </section>
          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-40">
              <div class="left-txt">
                <p>
                  <span>
                    In applications, one is often faced with over-determined
                    linear systems. 
                  </span>
                  <span>
                     For example, we may have a bunch of data
                    points that we have reasons to believe should fit on a
                    straight line. 
                  </span>
                  <span>
                    But real-life data points rarely match
                    predictions exactly.
                  </span>
                </p>
                <p>
                  The goal in this section is to develop a method for obtaining
                  the best fit or approximation of a specified kind to a given
                  set of data points.
                </p>
              </div>
              <div>
                <iframe scrolling="no" title="Chapter-12-2: Fitting line" src="https://www.geogebra.org/material/iframe/id/tmyev6ke/width/790/height/368/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/false" 
                width="790px" height="368px" style="border:0px;" allowfullscreen=""> </iframe>
              </div>
            </div>
            <br/><br/>
          </section>

          

          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  To this end, let $A\in M_{m,n}(\mathbb{R})$ and ${\bf b}\in
                  M_{m,1}(\mathbb{R})$.
                  <span class="fragment" data-fragment-index="1">
                    For $m>n$, the linear system described by $A{\bf x}={\bf b}$
                    is over-determined and does not in general have a solution.
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    However, we may be satisfied if we can find $\hat{\bf x}\in
                    M_{n,1}(\mathbb{R})$ such that $A\hat{\bf x}$ is as 'close'
                    to ${\bf b}$ as possible.
                  </span>
                  <span class="fragment" data-fragment-index="3">
                    That is, we seek to minimise $||{\bf b}-A{\bf x}||$ for
                    given $A$ and ${\bf b}$.
                  </span>
                  <span class="fragment" data-fragment-index="4">
                    For computational reasons, one usually minimises $||{\bf
                    b}-A{\bf x}||^2$ instead.
                  </span>
                </p>
                <p class="fragment" data-fragment-index="5">
                  A solution $\hat{\bf x}$ to this minimisation problem is
                  referred to as a
                  <strong>least squares solution</strong>.
                </p>
              </div>
            </div>
            <br/><br/><br/>
          </section>
          <section data-auto-animate>
            <h4 style="font-size:90%">12.2 Inconsistent linear systems</h4>
            <div class="size-40">
              <div class="left-txt">
                <p>
                  <span>
                    We seek to minimise $||{\bf b}-A{\bf x}||$ for
                    given $A$ and ${\bf b}$.
                  </span>
                  <span>
                    For computational reasons, one usually minimises $||{\bf
                    b}-A{\bf x}||^2$ instead. A solution $\hat{\bf x}$ to this minimisation problem is
                    referred to as a
                    <strong>least squares solution</strong>.
                  </span>
                </p>
                
              </div>
            </div>

            <div>
              <iframe scrolling="no" title="Chapter-12-2: Least squares error" src="https://www.geogebra.org/material/iframe/id/gkur3wqe/width/791/height/478/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/false" width="791px" height="478px" style="border:0px;" allofullscreen=""> </iframe>
            </div>
            <br/><br/>
            
          </section>

          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-65">
              <div class="left-txt">
                <p>
                  To find $\hat{\bf x}$, let us recall the
                  <strong>column space</strong>
                  of the matrix $A$.
                  <span class="fragment" data-fragment-index="0">
                    Let ${\bf a}_1,\ldots,{\bf a}_n\in M_{m,1}(\mathbb{R})$ be
                    the columns of $A$, i.e., $A = \left({\bf a_1} ~|~ {\bf a_2}
                    ~|~\cdots ~|~{\bf a_n}\right)$, then
                    $$\mathrm{Col}(A)=\mathrm{span}(\{{\bf a}_1,\ldots,{\bf
                    a}_n\}).$$
                  </span>
                </p>
              </div>
            </div>
            <div class="size-65">
              <div class="left-txt">
                <p>
                  <span class="fragment" data-fragment-index="1">
                    Let ${\bf x}\in M_{n,1}(\mathbb{R})$ then we have
                    \begin{align*} A{\bf x}& = A \begin{pmatrix} x_1\\ x_2\\
                    \vdots\\ x_n\end{pmatrix} = x_1{\bf a}_1+x_2{\bf a}_2+\cdots
                    + x_n{\bf a}_n,\quad x_k\in \R. \end{align*}
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    So $A{\bf x}$ is a linear combination of the columns of $A$,
                    i.e. a vector of the form $A{\bf x}$ is always a vector in
                    $\mathrm{Col}(A)$.
                  </span>
                </p>
              </div>
            </div>
            <br />
          </section>

          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span>
                    Let ${\bf x}\in M_{n,1}(\mathbb{R})$ then we have
                    \begin{align*} A{\bf x}& = A \begin{pmatrix} x_1\\ x_2\\
                    \vdots\\ x_n\end{pmatrix} = x_1{\bf a}_1+x_2{\bf a}_2+\cdots
                    + x_n{\bf a}_n,\quad x_k\in \R. \end{align*}
                  </span>
                  <span>
                    So $A{\bf x}$ is a linear combination of the columns of $A$,
                    i.e. a vector of the form $A{\bf x}$ is always a vector in
                    $\mathrm{Col}(A)$.
                  </span>
                </p>
              </div>
            </div>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  It follows that $A\hat{\bf x}\in\mathrm{Col}(A)$. We are thus
                  in the situation described in <a href="#/2/0/2">Section 12.1</a>.
                </p>
              </div>
            </div>
            <br />
          </section>

          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            
            <div class="size-70">
              <div class="left-txt">
                <p>
                  It follows that $A\hat{\bf x}\in\mathrm{Col}(A)$. We are thus
                  in the situation described in <a href="#/2/0/2">Section 12.1</a>.
                </p>
                <p>
                  For a finite-dimensional subspace $U\subset V $ and $\v\in V$ and
                  $\u \in U$,
                  <span class="fragment" data-fragment-index="0">
                    $\u = \mathrm{Proj}_{U}({\bf v})$ minimizes $\norm{\v-\u}^2$.
                  </span><br/><br/>
                  <span class="fragment" data-fragment-index="1">
                    Hence $U = \mathrm{Col}(A)$ and $\v= \mathbf b$
                    with all vectors in $U$ of the form $A \mathbf x$.
                  </span>
                  <span class="fragment" data-fragment-index="2"> 
                    Then $$\mathrm{Proj}_{\mathrm{Col}(A)}({\bf b})$$ is a vector in $\mathrm{Col}(A)$.
                  </span>
                </p>
              </div>
            </div>
            <br /><br/>
          </section>
          <section data-auto-animate>
            <h4 style="font-size:90%">12.2 Inconsistent linear systems</h4>
            
            <div class="size-50">
              <div class="left-txt">
                <p>
                  It follows that $A\hat{\bf x}\in\mathrm{Col}(A)$. We are thus
                  in the situation described in <a href="#/2/0/2">Section 12.1</a>.
                </p>
              </div>
            </div>
            <div>
              <iframe scrolling="no" title="Chapter-12-2: Column space A" src="https://www.geogebra.org/material/iframe/id/cyjwmnea/width/791/height/478/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/false" width="791px" height="478px" style="border:0px;" allofullscreen=""> </iframe>
            </div>
            <br /><br/>
          </section>

          <section data-auto-animate>
            <h4>12.2 Inconsistent linear systems</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  Consequently, we know that $$ A\hat{\bf
                  x}=\mathrm{Proj}_{\mathrm{Col}(A)}({\bf b}). $$
                  <span class="fragment" data-fragment-index="0">
                    But we are still faced with the task of disentangling
                    $\hat{\bf x}$.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Note that, although $A\hat{\bf
                    x}=\mathrm{Proj}_{\mathrm{Col}(A)}({\bf b})$ is
                    <em>uniquely</em> given in terms of $A$ and ${\bf b}$, its
                    solution $\hat{\bf x}$ need <em>not</em> be.
                  </span>
                </p>

                <p class="fragment" data-fragment-index="2">
                  <strong>Solution 3.</strong> For the current special
                  application, i.e. least squares solutions of linear systems,
                  we have a more direct (and simpler) method. In the following
                  we describe this method and give some examples.
                </p>
              </div>
            </div>
            <br /><br />
          </section>
        </section>

        <section>
          <h4 style="font-size:80%">12.3 $\mathrm{Col}(A)^\bot=N\left(A^T\right)$</h4>
          <div class="size-65">
            <div class="left-txt">
              <p>
                The orthogonal complement of the column space of $A$ is the null
                space of $A^T$.
              </p>
              <p class="fragment" data-fragment-index="0">
                <strong>Note:</strong>
                Recall that $\left(A\v\right)\cdot \u = \left(A\v\right)^T \u  = \v^T A^T \u = \v \cdot \left(A^T\u\right)$. $\quad(\star)$
              </p>
              <p class="fragment" data-fragment-index="1">
                <strong>Proof:</strong>
                <span class="fragment" data-fragment-index="2">
                  $\subseteq$ 
                </span>
                <span class="fragment" data-fragment-index="3">
                  - Let $\u\in \mathrm{Col}(A)^{\perp}$,
                </span>
                <span class="fragment" data-fragment-index="4">
                  then for $\v \in \R^n$
                </span>
              </p>
            </div>
            <div>
              <p class="fragment" data-fragment-index="5">
                $
                0 = \left(A \v\right)\cdot \u
                $
                <span class="fragment" data-fragment-index="6">
                  $
                  = \left(A \v\right)^T \u
                  $
                </span>
                <span class="fragment" data-fragment-index="7">
                  $
                  = \v^TA^T  \u
                  $
                </span>
                <span class="fragment" data-fragment-index="8">
                  $
                  = \v \cdot \left(A^T \u\right)  
                  $
                </span>
              </p>
            </div>
            <div>
              <p class="fragment" data-fragment-index="9">
                Set $\v = A^T\u$,
                <span class="fragment" data-fragment-index="10">
                  then $0 =  \left(A^T \u\right) \cdot   \left(A^T \u\right)  $
                </span>
                <span class="fragment" data-fragment-index="11">
                 $\; \Ra\; A^T\u = \mathbf 0  $
                </span>
                <span class="fragment" data-fragment-index="12">
                 $\; \Ra \;\u \in N\left(A^T\right)  $
                </span>
                <span class="fragment" data-fragment-index="13">
                 $\; \Ra \; \mathrm{Col}(A)^{\perp} \subseteq N\left(A^T\right) .$
                </span>
              </p>
            </div>
            <div class="left-txt">
              <p class="fragment" data-fragment-index="14">
                
                $\supseteq$
                
                <span class="fragment" data-fragment-index="15">
                  - Let $\u\in N\left(A^T\right)$,
                </span>
                <span class="fragment" data-fragment-index="16">
                  then for $A^T \u = \mathbf 0$.
                </span>
                <span class="fragment" data-fragment-index="17">
                  Thus for all  $\v \in \R^n$ we have 
                </span>
              </p>
            </div>
            <div>
              <p class="fragment" data-fragment-index="18">
                $
                0 =\v \cdot \mathbf 0
                $
                <span class="fragment" data-fragment-index="19">
                  $
                  = \v \cdot \left(A^T \u\right)
                  $
                </span>
                <span class="fragment" data-fragment-index="20">
                  $
                  = \v^TA^T  \u
                  $
                </span>
                <span class="fragment" data-fragment-index="21">
                  $
                  = \left(A\v\right)^T \u
                  $
                </span>
                <span class="fragment" data-fragment-index="22">
                  $
                  = \left(A\v\right) \cdot \u
                  $
                </span>
              </p>
            </div>
            <div>
              <p class="fragment" data-fragment-index="23">
                So $\u$ is perpendicular to all vectors of the form $A \v\in \mathrm{Col}(A)$
                <span class="fragment" data-fragment-index="24">
                  $
                  \; \Ra \;\u \in \mathrm{Col}(A)^{\perp}
                  $
                </span>
                <span class="fragment" data-fragment-index="25">
                  $
                  \; \Ra \;\mathrm{Col}(A)^{\perp} \supseteq N\left(A^T\right).
                  $
                </span>
                <span class="fragment" data-fragment-index="26">
                  $
                  \quad \blacksquare
                  $
                </span>
              </p>
            </div>
          </div>
          <br/>
        </section>

        <section>
          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span class="fragment" data-fragment-index="0">
                    Recall from <a href=chapter-11.html#/5/0/1>Section 11.4</a> that
                    $\bfv-\mathrm{Proj}_U(\bfv)=\mathrm{Proj}_{U^\bot}(\bfv)\in
                    U^\bot$ for any finite-dimensional subspace $U$ of $V$ and
                    $\bfv\in V$.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Since $A\hat{\bf x}=\mathrm{Proj}_{\mathrm{Col}(A)}({\bf
                    b})$, we thus have $$ {\bf b}-A\hat{\bf
                    x}\in\mathrm{Col}(A)^\bot. $$
                  </span>
                  
                </p>
              </div>
            </div>
            
            <br /><br/><br/><br/>
          </section>
          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-50">
              <div class="left-txt">
                <p>
                  <span>
                    Recall from <a href=chapter-11.html#/5/0/1>Section 11.4</a> that
                    $\bfv-\mathrm{Proj}_U(\bfv)=\mathrm{Proj}_{U^\bot}(\bfv)\in
                    U^\bot$ for any finite-dimensional subspace $U$ of $V$ and
                    $\bfv\in V$.
                  </span>
                  <span>
                    Since $A\hat{\bf x}=\mathrm{Proj}_{\mathrm{Col}(A)}({\bf
                    b})$, we thus have $$ {\bf b}-A\hat{\bf
                    x}\in\mathrm{Col}(A)^\bot. $$
                  </span>
                  
                </p>
              </div>
            </div>
            <div>
              <img width="85%" src="images/chapter-12/chapter-12-2-perpu.png">
            </div>
            <br /><br/>
          </section>
          
          
          
          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span>
                    Recall from <a href=chapter-11.html#/5/0/1>Section 11.4</a> that
                    $\bfv-\mathrm{Proj}_U(\bfv)=\mathrm{Proj}_{U^\bot}(\bfv)\in
                    U^\bot$ for any finite-dimensional subspace $U$ of $V$ and
                    $\bfv\in V$.
                  </span>
                  <span>
                    Since $A\hat{\bf x}=\mathrm{Proj}_{\mathrm{Col}(A)}({\bf
                    b})$, we thus have $$ {\bf b}-A\hat{\bf
                    x}\in\mathrm{Col}(A)^\bot. $$
                  </span>
                </p>
                <p><span>
                  Because $\mathrm{Col}(A)^\bot=N\left(A^T\right)$, it follows that
                  $A^T({\bf b}-A\hat{\bf x})={\bf 0}$, so $$ A^T\!A\hat{\bf
                  x}=A^T{\bf b}. $$
                  </span>
                </p>
              </div>
            </div>
            <br /><br /><br/><br/>
          </section>

          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  Since $A\in M_{m,n}(\mathbb{R})\; \Rightarrow\; A^T\!A\in
                  M_{n,n}(\mathbb{R})$, the equation $A^T\!A\hat{\bf x}=A^T{\bf
                  b}$ can always be solved for $\hat{\bf x}$, for example by
                  Gaussian elimination.

                  <span class="fragment" data-fragment-index="0">
                    However, the solution need not be unique.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Indeed, the solution is unique if and only if $A^T\!A$ is
                    invertible, in which case $$ \hat{{\bf
                    x}}=\left(A^T\!A\right)^{-1}A^T{\bf b}. $$
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    A key to determining whether $A^T\!A$ is invertible is the
                    following result:
                  </span>
                </p>
              </div>
              <div class="fragment" data-fragment-index="3">
                <p>
                  $\{{\bf a}_1,\ldots,{\bf a}_n\}$ is linearly independent
                  $\quad\Longleftrightarrow\quad$ $A^TA$ is invertible.
                </p>
              </div>
            </div>
            <br /><br /><br/>
          </section>

          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-50">
              <div>
                <p>
                  <strong>$\{{\bf a}_1,\ldots,{\bf a}_n\}$ is linearly independent
                    $\quad\Longleftrightarrow\quad$ $A^TA$ is invertible.</strong>
                </p>
              </div>
            </div>
            <div class="size-70">
              <div class="left-txt">
                <p>
                 
                  <span>
                    To prove this result, first we will prove the following 
                  </span>
                </p>
                <p>
                  <strong>Lemma:</strong> $N(A) = N\left(A^TA\right).$
                </p>
                <p>
                  <span class="fragment" data-fragment-index="0">
                    <strong>Proof:</strong> "$\subseteq$"  
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    - For $\v \in N(A) $
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    $\;\Ra\; A\v = \mathbf 0$
                  </span>
                  <span class="fragment" data-fragment-index="3">
                    $\;\Ra\; A^T\v = \mathbf 0$
                  </span>
                  <span class="fragment" data-fragment-index="4">
                    $\;\Ra\; \v \in  N\left(A^TA\right).$
                  </span>
                </p>
                <p>
                  <span class="fragment" data-fragment-index="5">
                    "$\supseteq$" 
                  </span>
                  <span class="fragment" data-fragment-index="6">
                    - For $\v \in  N\left(A^TA\right) $ 
                  </span>
                  <span class="fragment" data-fragment-index="7">
                    $\;\Ra\; A^TA\v = \mathbf 0$
                  </span>
                  
                </p>
              </div>
              <div>
                <p>
                  <span class="fragment" data-fragment-index="8">
                    $\;\Ra\;  \mathbf 0 = \v \cdot \mathbf 0$
                  </span>
                  <span class="fragment" data-fragment-index="9">
                    $= \v \cdot \left(A^TA \v\right)$
                  </span>
                  <span class="fragment" data-fragment-index="10">
                    $= \v^T A^T A \v$
                  </span>
                  <span class="fragment" data-fragment-index="11">
                    $=  \left(A\v \right)^T A \v$
                  </span>
                  <span class="fragment" data-fragment-index="12">
                    $=  \left(A\v \right)\cdot  \left(A\v \right)$
                  </span>
                </p>
                <p>
                  <span class="fragment" data-fragment-index="13">
                    $\;\Ra\; A\v = \mathbf 0$
                  </span>
                  <span class="fragment" data-fragment-index="14">
                    $\;\Ra\; \v = \mathbf 0$
                  </span>
                  <span class="fragment" data-fragment-index="15">
                    $\;\Ra\; \v \in N(A).$
                  </span>
                  <span class="fragment" data-fragment-index="16">
                    $\quad \blacksquare$
                  </span>
                </p>
              </div>
            </div>
            <br /><br /><br/>
          </section>

          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-70">
              <div>
                <p>
                  <strong>$\{{\bf a}_1,\ldots,{\bf a}_n\}$ is linearly independent
                    $\quad\Longleftrightarrow\quad$ $A^TA$ is invertible.</strong>
                </p>
              </div>
            </div>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span>
                    <strong>Proof:</strong>
                  </span>
                  <span class="fragment" data-fragment-index="0">
                    The set $\left\{{\bf a}_1,\ldots,{\bf a}_n\right\}$ is linearly independent
                    if and only if
                  </span>
                </p>
                
              </div>
              <div>
                <p>
                  <span class="fragment" data-fragment-index="1">
                    $N(A) = \{\mathbf 0\}$
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    $\;\iff \; N\left(A^TA\right) = \{\mathbf 0\}\;$
                  </span>
                  <span class="fragment" data-fragment-index="3">
                    by <a href="#/5/4/16">previous Lemma</a>
                  </span>
                </p>
                <p>
                  <span class="fragment" data-fragment-index="4">
                    $\;\iff \;$ $A^TA$ is invertible.
                  </span>
                  <span class="fragment" data-fragment-index="5">
                    $\quad \blacksquare$
                  </span>
                </p>
              </div>
            </div>
            <br /><br /><br/><br/><br/>
          </section>

          <section data-auto-animate>
            <h4>12.4 Solving for $\hat{\bf x}$</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span style="font-size:110%">
                    🤔 But how does that help?
                  </span>
                  <br/><br/>
                  <span class="fragment" data-fragment-index="0">
                    Well, recall that $m>n$ so that $A$ may be describing an
                    over-determined linear system.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    In fact, in case the linear system encodes experimental or
                    empirical data, $m$ is likely to be much larger than $n$.
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    The columns of $A$ are then very likely to be linearly
                    independent, and $A^TA$ would indeed be invertible.
                  </span>
                </p>
              </div>
            </div>
            <br /><br /><br /><br />
          </section>
        </section>

        <section>
          <section data-auto-animate>
            <h4>12.5 Fitting a curve to data</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <span class="fragment" data-fragment-index="0">
                    Experiments yield data (assume $x_i$ distinct and exact) $$
                    (x_1, y_1),\;(x_2,y_2),\;\cdots, \;(x_n, y_n) $$ which
                    include measurement error.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Theory may predict polynomial relation between $x$ and $y$.
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    But experimental data points rarely match theoretical
                    predictions exactly.
                  </span>
                </p>

                <p class="fragment" data-fragment-index="3">
                  We seek
                  <em>a least squares polynomial function of best fit</em> (e.g.
                  least squares line of best fit or regression line).
                </p>
              </div>
            </div>
            <br /><br />
          </section>

          <section data-auto-animate>
            <h4>12.5 Fitting a curve to data</h4>
            <div class="size-60">
              <div class="left-txt">
                <p>
                  We seek
                  <em>a least squares polynomial function of best fit</em> (e.g.
                  least squares line of best fit or regression line).
                </p>
              </div>
              <div><iframe scrolling="no" title="Chapter-12-5: Fitting curve to data" 
                src="https://www.geogebra.org/material/iframe/id/n3qqs9ct/width/789/height/373/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/true" 
                width="789px" height="373px" style="border:0px;" allowfullscreen=""> </iframe>
              </div>
            </div>
            <br /><br />
          </section>

          <section data-auto-animate>
            <h4>12.5 Fitting a curve to data</h4>
            <div class="size-70">
              <div class="left-txt">
                <p>
                  <strong>Example: Quadratic fit.</strong>
                  <span class="fragment" data-fragment-index="0">
                    Suppose some physical system is modeled by a quadratic
                    function $p(t)$.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Data in the form $(t,p(t))$ have been recorded as $$ (1,5),\
                    (2,2),\ (4,7),\ (5,10). $$ Find the least squares
                    approximation for $p(t)$.
                  </span>
                </p>
              </div>
            </div>
            <br /><br /><br /><br /><br /><br />

          </section>
          <section data-auto-animate>
            <h4>12.5 Fitting a curve to data</h4>
            <div class="size-50">
              <div class="left-txt">
                <p>
                  <strong>Example: Quadratic fit.</strong>
                  <span>
                    Suppose some physical system is modeled by a quadratic
                    function $p(t)$.
                  </span>
                  <span>
                    Data in the form $(t,p(t))$ have been recorded as $$ (1,5),\
                    (2,2),\ (4,7),\ (5,10). $$ Find the least squares
                    approximation for $p(t)$.
                  </span>
                </p>
              </div>
            </div>
            <div class="size-60">
              <div class="left-txt">
                <p>
                  Consider the quadratic function $p(t) = a_0 + a_1 t + a_2 t^2$. 
                  <span class="fragment" data-fragment-index="0">
                    To find the least
                  squares approximation we need to solve the linear system:
                  </span>
                </p>
                <div class="r-stack" style="margin-top:-20px">
                  <p class="fragment fade-in-then-out" data-fragment-index="0">
                    <span>
                      \[
                    \left\{
                    \begin{array}{ccccccc}
                    a_0& +& a_1 (1)& + & a_2 (1)^2 & = & 5 \\
                    a_0& + &a_1 (2) &+ &a_2 (2)^2 & = & 2 \\
                    a_0 &+& a_1 (4) &+ &a_2 (4)^2 & = & 7 \\
                    a_0 &+& a_1 (5)& +& a_2 (5)^2 & = & 10 \\
                    \end{array}  
                    \right.
                    \]
                    </span>
                  </p>
                  <p class="fragment fade-in-then-out" data-fragment-index="1">
                    <span>
                      \[
                    \left\{
                    \begin{array}{ccrcrcc}
                    a_0& +& a_1 & + & a_2  & = & 5 \\
                    a_0& + & 2a_1  &+ & 4a_2  & = & 2 \\
                    a_0 &+& 4a_1  &+ & 16a_2  & = & 7 \\
                    a_0 &+& 5a_1 & +& 25a_2  & = & 10 \\
                    \end{array}  
                    \right.
                    \]
                    </span>
                  </p>
                  <p class="fragment fade-in-then-out" data-fragment-index="2">
                    <span>
                      \[
                    \left(
                    \begin{array}{ccc}
                    1 &  1 &  1  \\
                    1 &  2 &  4  \\
                    1 &  4 &  16  \\
                    1 &  5 &  25 \\
                    \end{array}  
                    \right)
                    \left(
                    \begin{array}{c}
                     a_0 \\
                     a_1 \\
                     a_2 \\
                    \end{array}  
                    \right)
                    =
                    \left(
                    \begin{array}{c}
                     5 \\
                     2 \\
                     7 \\
                     10 \\
                    \end{array}  
                    \right)
                    \]
                    </span>
                  </p>
                  <p class="fragment fade-in" data-fragment-index="3">
                    <span>
                      \[
                    \underbrace{\left(
                    \begin{array}{ccc}
                    1 &  1 &  1  \\
                    1 &  2 &  4  \\
                    1 &  4 &  16  \\
                    1 &  5 &  25 \\
                    \end{array}  
                    \right)}_{A}
                    \underbrace{\left(
                    \begin{array}{c}
                     a_0 \\
                     a_1 \\
                     a_2 \\
                    \end{array}  
                    \right)}_{\mathbf x}
                    =
                    \underbrace{\left(
                    \begin{array}{c}
                     5 \\
                     2 \\
                     7 \\
                     10 \\
                    \end{array}  
                    \right)}_{\mathbf b}
                    \]
                    </span>
                  </p>
                  
                </div>
                
              </div>
            </div>
            <br /><br />
          </section>
          <section data-auto-animate>
            <h4>12.5 Fitting a curve to data</h4>
            <div class="size-40">
              <div class="left-txt">
                <p>
                  <strong>Example: Quadratic fit.</strong>
                  <span>
                    Suppose some physical system is modeled by a quadratic
                    function $p(t)$.
                  </span>
                  <span>
                    Data in the form $(t,p(t))$ have been recorded as $$ (1,5),\
                    (2,2),\ (4,7),\ (5,10). $$ Find the least squares
                    approximation for $p(t)$.
                  </span>
                </p>
              </div>
            </div>
            <div class="size-60">
              <div class="left-txt">
                
                <div class="r-stack size-50" style="margin-top:-30px">
                  <p>
                    <span>
                      \[
                    \underbrace{\left(
                    \begin{array}{ccc}
                    1 &  1 &  1  \\
                    1 &  2 &  4  \\
                    1 &  4 &  16  \\
                    1 &  5 &  25 \\
                    \end{array}  
                    \right)}_{A}
                    \underbrace{\left(
                    \begin{array}{c}
                     a_0 \\
                     a_1 \\
                     a_2 \\
                    \end{array}  
                    \right)}_{\mathbf x}
                    =
                    \underbrace{\left(
                    \begin{array}{c}
                     5 \\
                     2 \\
                     7 \\
                     10 \\
                    \end{array}  
                    \right)}_{\mathbf b}
                    \]
                    </span>
                  </p>
                  
                </div>
                <p style="margin-top:-5px">
                  So we have $A\mathbf x = \mathbf b$. 
                  <span class="fragment" data-fragment-index="0">
                    This is an  over-determined and inconsistent system 
                  but the columns of $A$ are linearly independent.
                  </span>
                  <span class="fragment" data-fragment-index="1">
                    Then $A^TA$ is invertible.
                  </span>
                  <span class="fragment" data-fragment-index="2">
                    Thus the least squares approximation is 
                  </span>
                  
                </p>
              </div>
              <div>
                <p >
                  <span class="fragment" data-fragment-index="2">
                    $\ds \hat{\mathbf x} = \left(A^T A\right)^{-1}A^T \mathbf b$
                  </span>
                  <span class="fragment" data-fragment-index="3">
                  $= 
                  \left(
                  \begin{array}{r}
                   8 \\
                   -\dfrac{9}{2} \\
                   1 \\
                  \end{array}  
                  \right)
                  $
                  </span>
                  <span class="fragment" data-fragment-index="4">
                  $\;\,\Ra \;\, p(t) = 8 - \dfrac{9}{2}t+ t^2.
                  $
                  </span>
                </p>
              </div>
            </div>
            <br /><br/>
          </section>

          <section data-auto-animate id="quadratic-fit">
            <h4>Quadratic fit</h4>
            <div>
              <iframe scrolling="no" title="Chapter-12-5: Quadratic fit" 
              src="https://www.geogebra.org/material/iframe/id/wuguwkpk/width/700/height/374/border/888888/sfsb/true/smb/false/stb/false/stbh/false/ai/false/asb/false/sri/true/rc/false/ld/false/sdz/true/ctl/true" 
              width="700px" height="374px" style="border:0px;" allowfullscreen=""> </iframe>
              <p style="font-size:18px">
                <strong>Data:</strong> $(1,5),\
                    (2,2),\ (4,7),\ (5,10)$.
              </p>
            </div>
            <br/><br/>
          </section>

        </section>

        <section>
          <h4>Credits</h4>
          <div include-html="creditsb.html"></div>
        </section>

        <!-- Ends sections -->
      </div>
    </div>

    <script src="dist/reveal.js"></script>
    <script src="plugin/math/math.js"></script>
    <script src="plugin/highlight/highlight.js"></script>
    <script src="plugin/chalkboard/plugin.js"></script>
    <script src="plugin/customcontrols/plugin.js"></script>
    <script src="plugin/menu/menu.js"></script>
    <script src="mySetup/setup.js"></script>

    <script>
      includeHTML();
    </script>
  </body>
</html>
